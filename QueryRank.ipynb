{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# compute idf of a token  *idfs are the same for the same token, only depend on the df and N in docs\n",
    "def calculate_idf(N, df):\n",
    "    idf = math.log((N + 0.1) / (df + 0.1))\n",
    "    return idf\n",
    "\n",
    "#comupte tfidf by tf and idf    \n",
    "def calculate_tfidf(tf, idf):   \n",
    "    return tf*idf\n",
    "\n",
    "#compute cosine_similarity bewteen query vector and doc vector,support any lenth\n",
    "def calculate_cosine_similarity(vector_query,vector_doc):\n",
    "    dot_product = sum(q * d for q, d in zip(vector_query, vector_doc))\n",
    "    norm_query = math.sqrt(sum(q * q for q in vector_query))\n",
    "    norm_doc = math.sqrt(sum(d * d for d in vector_doc))\n",
    "\n",
    "    return dot_product / ((norm_query * norm_doc)+0.1)\n",
    "\n",
    "#create query vector, query is like \"fox dog\", idf_dict is like {\"fox\":idf of fox,\"dog\":idf of dog}\n",
    "def create_query_vector(query, idf_dict):\n",
    "    #tokenize the query using our tokenizor!!!\n",
    "    words = query.split()\n",
    "    query_vector = []\n",
    "\n",
    "    # Calculate TF-IDF for each word in the query\n",
    "    for word in words:\n",
    "        tf = words.count(word) / len(words)\n",
    "        idf = idf_dict.get(word, 0)# Get IDF from the IDF dictionary (assuming it's already computed, you can just store df and N, and use calculate_idf to compute idf)\n",
    "        tf_idf = tf * idf\n",
    "        query_vector.append(tf_idf)\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "#create doc vectors, like {'doc1': [0.2556430078148932, 0.10177675964835226], 'doc2': [0.0, 0.30533027894505677]}\n",
    "def create_doc_vector(query, query_vector, inverted_index_tfs, idf_dict):\n",
    "    doc_vectors = {}\n",
    "\n",
    "    # Iterate over tokens in the query\n",
    "    # also replace with our tokenizors!!!\n",
    "    for token in query.split():\n",
    "        # Check if the token exists in the inverted index postings\n",
    "        if token in inverted_index_tfs:\n",
    "            postings = inverted_index_tfs[token]\n",
    "\n",
    "            # Iterate over the document IDs and TF values in the postings\n",
    "            for doc_id, tf in postings.items():\n",
    "                # Check if the document ID already has a vector\n",
    "                if doc_id not in doc_vectors:\n",
    "                    doc_vectors[doc_id] = [0] * len(query_vector)\n",
    "\n",
    "                # Set the TF-IDF value in the document vector based on the query vector index and IDF\n",
    "                query_index = query.split().index(token)\n",
    "                tfidf = tf * idf_dict[token]\n",
    "                doc_vectors[doc_id][query_index] = tfidf\n",
    "\n",
    "    return doc_vectors\n",
    "\n",
    "#compute cosine_similarity and return dictionary of {docid:cosine_similarity},already sorted, can make changes for like top 10\n",
    "def create_cs_doc(query_vector,doc_vectors):\n",
    "    doc_similarities={}\n",
    "    for docid,vetcor in doc_vectors.items():\n",
    "        doc_similarities[docid]=calculate_cosine_similarity(query_vector,vetcor)\n",
    "    doc_similarities = {k: v for k, v in sorted(doc_similarities.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def stemQuery(query:str) -> list:\n",
    "    stemmer = PorterStemmer()\n",
    "    queryList = list()\n",
    "    line = query.strip()\n",
    "    if line != '':\n",
    "        for aToken in re.split('[^a-z0-9]', line.lower()):\n",
    "            if (aToken != ''):\n",
    "                token = stemmer.stem(aToken)\n",
    "                queryList.append(token)\n",
    "    return queryList\n",
    "\n",
    "def getDf(term:str) -> int:\n",
    "    df = pd.read_hdf('DevHDF5/TermMeta.hdf5', term[0])\n",
    "    if term in df:\n",
    "        return df[term]['df']\n",
    "    return 0\n",
    "\n",
    "def getTermDocId(term:str) -> int:\n",
    "    df = pd.read_hdf('DevHDF5/TermMeta.hdf5', term[0])\n",
    "    if term in df:\n",
    "        return df[term]['docIds']\n",
    "    return 0\n",
    "\n",
    "def getDocLen(docId:int) -> int:\n",
    "    def getDocKey(docId:int, N = 60000):\n",
    "        for doc in range(0,N+1,5000):\n",
    "            if docId <= doc:\n",
    "                return str(doc)\n",
    "        \n",
    "    df = pd.read_hdf('DevHDF5/DocId.hdf5', getDocKey(docId))\n",
    "    return df['docLen'][docId]\n",
    "\n",
    "def getTf(term:str):\n",
    "    df = pd.read_hdf('DevHDF5/Terms.hdf5', 'terms')\n",
    "    if term in df['Terms']:\n",
    "        return json.loads(df['Terms'][term])\n",
    "    return {}\n",
    "\n",
    "def getDocIds(queryList:list) -> set:\n",
    "    unionSet = set()\n",
    "    for word in queryList:\n",
    "        docSet = getTermDocId(word)\n",
    "        if len(unionSet) == 0:\n",
    "            unionSet = docSet\n",
    "        else:\n",
    "            unionSet = unionSet.intersection(docSet)\n",
    "    return unionSet\n",
    "\n",
    "def getInvertedTf(term:str, docIdList:list[int]) -> dict:\n",
    "    resultTf = {}\n",
    "    resultTf[term] = {}\n",
    "    tfDict = getTf(term)\n",
    "    for docId in docIdList:\n",
    "        docIdStr = str(docId)\n",
    "        if docIdStr in tfDict:\n",
    "            resultTf[term][f'doc{docId}'] = tfDict[docIdStr] / getDocLen(docId)\n",
    "    return resultTf\n",
    "\n",
    "def getIdfDict(queryList:list[str], N = 55382) -> dict:\n",
    "    idfDict = {}\n",
    "    for word in queryList:\n",
    "        wordDf = getDf(word)\n",
    "        idfDict[word] = calculate_idf(N, wordDf)\n",
    "    return idfDict\n",
    "\n",
    "def getInvertedTfDict(queryList:list, docList:list[int]) -> dict:\n",
    "    resultTf = {}\n",
    "    for word in queryList:\n",
    "        resultTf.update(getInvertedTf(word,docList))\n",
    "    return resultTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qWords = stemQuery(\"fox dog and\") # ['fox', 'dog', 'and']\n",
    "docIds = getDocIds(qWords) #{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ... }\n",
    "idf_dict = getIdfDict(qWords) #{'fox': 7.007990707741848, 'dog': 5.623194474242979, 'and': 4.930297199969477}\n",
    "inverted_index_tfs = getInvertedTfDict(qWords, docIds) #{'fox': {'doc1': 0.1111111111111111, 'doc2': 0, 'doc3': 0.3333333333333333}, 'dog': {'doc1': 0.1111111111111111, .... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc3': 0.9915046317016729, 'doc1': 0.8517176154476556, 'doc2': 0.6518100708682779}\n"
     ]
    }
   ],
   "source": [
    "Q = \"fox dog and\"\n",
    "N = 55382 #total docs\n",
    "df_fox = 50 \n",
    "df_dog = 200 \n",
    "df_and = 400\n",
    "idf_dict = {\"fox\":calculate_idf(N, df_fox),\"dog\":calculate_idf(N, df_dog),\"and\":calculate_idf(N, df_and)}#idf of each word\n",
    "\n",
    "inverted_index_tfs = {\"fox\":{\"doc1\":(1/9),\"doc2\":0,\"doc3\":1/3},\"dog\":{\"doc1\":(1/9),\"doc2\":(1/3),\"doc3\":1/3},\"and\":{\"doc1\":(0/9),\"doc2\":(1/9),\"doc3\":1/3}}#tf of each word in different docs\n",
    "\n",
    "query_vector = create_query_vector(Q,idf_dict)\n",
    "doc_vectors = create_doc_vector(Q, query_vector, inverted_index_tfs, idf_dict)\n",
    "cs = create_cs_doc(query_vector,doc_vectors)\n",
    "#RESULT:{'doc3': 0.8729249902928188, 'doc1': 0.6923695754433984, 'doc2': 0.26863855424377914} the reason that score of the doc3 is not 1 becuase I add 0.1 to each for avoiding divide 0 errors\n",
    "print(cs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
