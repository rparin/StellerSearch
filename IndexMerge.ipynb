{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "def getFpDict(count):\n",
    "    fpDict = {}\n",
    "    for i in range(1,count+1):\n",
    "        with open(f'Data/Posting{i}.txt', \"r+\") as fp:\n",
    "            while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "                fpPos = fp.tell()\n",
    "                line = fp.readline().split('-')\n",
    "                term = line[0]\n",
    "                if term not in fpDict:\n",
    "                    fpDict[term] = {}\n",
    "                fpDict[term][i] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getTermInfo(term:str, fpDict:dict):\n",
    "    termInfo = None\n",
    "    if term not in fpDict: return termInfo\n",
    "    for count in fpDict[term]:\n",
    "        with open(f'Data/Posting{count}.txt', \"r+\") as fp:\n",
    "            fp.seek(fpDict[term][count])\n",
    "            tInfo = fp.readline().strip().split('-')[1]\n",
    "            tInfo = json.loads(tInfo)\n",
    "            if not termInfo:\n",
    "                termInfo = tInfo\n",
    "            else:\n",
    "                #Merge\n",
    "                termInfo['df'] += tInfo['df']\n",
    "                termInfo['wTf'].update(tInfo['wTf'])\n",
    "                termInfo['docIds'].extend(tInfo['docIds'])\n",
    "    return termInfo\n",
    "\n",
    "def getFpIdDict():\n",
    "    fpDict = {}\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split('>')\n",
    "            docId = line[0]\n",
    "            fpDict[int(docId)] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getUrl(docId, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['url']\n",
    "    \n",
    "def getDocLen(docId:int, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['docLen']\n",
    "    \n",
    "def calculate_idf(df, N = 55382):\n",
    "    idf = math.log((N + 0.1) / (df + 0.1))\n",
    "    return idf\n",
    "    \n",
    "def writeIndex(fpDict, docIdDict):\n",
    "    for term in fpDict:\n",
    "        termInfo = getTermInfo(term,fpDict)\n",
    "        idfDict = {}\n",
    "        idf = calculate_idf(termInfo['df'])\n",
    "\n",
    "        for docId in termInfo['wTf']:\n",
    "            idfDict[docId] = round((termInfo['wTf'][docId] / getDocLen(int(docId), docIdDict)) * idf,5)\n",
    "\n",
    "        with open(f'Data/Index.txt', \"a\") as fp:\n",
    "            fp.write(f'{term}-{json.dumps(idfDict)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idDict = getFpIdDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpDict = getFpDict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeIndex(fpDict, idDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
