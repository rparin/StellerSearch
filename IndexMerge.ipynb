{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from heapq import heapify, heappush, heappop\n",
    "\n",
    "def getFpDict(count):\n",
    "    fpDict = {}\n",
    "    for i in range(1,count+1):\n",
    "        with open(f'Data/Posting{i}.txt', \"r\") as fp:\n",
    "            while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "                fpPos = fp.tell()\n",
    "                line = fp.readline().split('-')\n",
    "                term = line[0]\n",
    "                if term not in fpDict:\n",
    "                    fpDict[term] = {}\n",
    "                fpDict[term][i] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getTermInfo(term:str, fpDict:dict):\n",
    "    termInfo = None\n",
    "    if term not in fpDict: return termInfo\n",
    "    for count in fpDict[term]:\n",
    "        with open(f'Data/Posting{count}.txt', \"r\") as fp:\n",
    "            fp.seek(fpDict[term][count])\n",
    "            tInfo = fp.readline().strip().split('-')[1]\n",
    "            tInfo = json.loads(tInfo)\n",
    "            if not termInfo:\n",
    "                termInfo = tInfo\n",
    "            else:\n",
    "                #Merge\n",
    "                termInfo['df'] += tInfo['df']\n",
    "                termInfo['wTf'].update(tInfo['wTf'])\n",
    "                termInfo['docIds'].extend(tInfo['docIds'])\n",
    "    return termInfo\n",
    "\n",
    "def getFpIdDict():\n",
    "    fpDict = {}\n",
    "    with open(f'Data/docId.txt', \"r\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split('>')\n",
    "            docId = line[0]\n",
    "            fpDict[int(docId)] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getUrl(docId, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['url']\n",
    "    \n",
    "def getDocLen(docId:int, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['docLen']\n",
    "    \n",
    "def calculate_idf(df, N = 55382):\n",
    "    try:\n",
    "        idf = math.log((N) / (df))\n",
    "    except ZeroDivisionError as e:\n",
    "        idf = 0\n",
    "    return idf\n",
    "\n",
    "def calculate_tf_idf(tf, idf):\n",
    "    return ((1 + math.log(tf)) * idf)\n",
    "    \n",
    "def writeIndex(fpDict, docIdDict):\n",
    "    for term in fpDict:\n",
    "        termInfo = getTermInfo(term,fpDict)\n",
    "        idf = calculate_idf(termInfo['df'])\n",
    "        termInfoDict = {'idf':idf, 'cList':[]}\n",
    "        cList = []; heapify(cList)\n",
    "\n",
    "        for docIdStr in termInfo['wTf']:\n",
    "            docLen = getDocLen(int(docIdStr), docIdDict)\n",
    "            tf = termInfo['wTf'][docIdStr]\n",
    "            tf_idf = calculate_tf_idf(tf/math.sqrt(docLen), idf) \n",
    "            termInfoDict[docIdStr] = tf_idf\n",
    "            if len(cList) < 20:\n",
    "                heappush(cList, (tf_idf,docIdStr))\n",
    "            else:\n",
    "                minTf_Idf = cList[0][0]\n",
    "                if tf_idf > minTf_Idf:\n",
    "                    heappop(cList)\n",
    "                    heappush(cList, (tf_idf,docIdStr))\n",
    "\n",
    "        for tup in cList:\n",
    "            termInfoDict['cList'].append(tup[1])\n",
    "            \n",
    "        with open(f'Data/Index.txt', \"a\") as fp:\n",
    "            fp.write(f'{term}>{json.dumps(termInfoDict)}\\n')\n",
    "\n",
    "def getFpIndex():\n",
    "    fpDict = {}\n",
    "    with open(f'Data/Index.txt', \"r\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split('>')\n",
    "            term = line[0]\n",
    "            fpDict[term] = fpPos\n",
    "    return fpDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idDict = getFpIdDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpDict = getFpDict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeIndex(fpDict, idDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpIndex = getFpIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def writeFeather(fileName:str, colName:str, dictObj: dict):\n",
    "    df = pd.DataFrame.from_dict({colName:dictObj})\n",
    "    feather.write_feather(df, f'Data/{fileName}.feather')\n",
    "\n",
    "writeFeather('indexFp', 'fp', fpIndex)\n",
    "writeFeather('docIdFp', 'fp', idDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "import json\n",
    "\n",
    "def getFpDataframe(fileName:str):\n",
    "    fpDf = None\n",
    "    with open(f'Data/{fileName}.feather', 'rb') as f:\n",
    "        fpDf = feather.read_feather(f)\n",
    "    return fpDf\n",
    "\n",
    "def getTermData(term:str, fpDf):\n",
    "    termInfo = None\n",
    "    if term not in fpDf['fp']: return termInfo\n",
    "    with open(f'Data/Index.txt', \"r\") as indexFile:\n",
    "        indexFile.seek(fpDf['fp'][term])\n",
    "        tInfo = indexFile.readline().strip().split('>')[1]\n",
    "        return json.loads(tInfo) \n",
    "\n",
    "def getDocData(docId:int, fpDf):\n",
    "    docInfo = None\n",
    "    if docId not in fpDf['fp']: return docInfo\n",
    "    with open(f'Data/docId.txt', \"r\") as indexFile:\n",
    "        indexFile.seek(fpDf['fp'][docId])\n",
    "        tInfo = indexFile.readline().strip().split('>')[1]\n",
    "        return json.loads(tInfo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termFpDf = getFpDataframe('indexFp')\n",
    "docIdFpDf = getFpDataframe('docIdFp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getTermData('hello', termFpDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getDocData(1, docIdFpDf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
