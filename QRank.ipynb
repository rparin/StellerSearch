{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "fileName = 'DataWt'\n",
    "\n",
    "def getFpDict(count):\n",
    "    fpDict = {}\n",
    "    for i in range(1,count+1):\n",
    "        with open(f'{fileName}/Posting{i}.txt', \"r+\") as fp:\n",
    "            while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "                fpPos = fp.tell()\n",
    "                line = fp.readline().split('-')\n",
    "                term = line[0]\n",
    "                if term not in fpDict:\n",
    "                    fpDict[term] = {}\n",
    "                fpDict[term][i] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getTermInfo(term:str, fpDict:dict):\n",
    "    termInfo = None\n",
    "    if term not in fpDict: return termInfo\n",
    "    for count in fpDict[term]:\n",
    "        with open(f'{fileName}/Posting{count}.txt', \"r+\") as fp:\n",
    "            fp.seek(fpDict[term][count])\n",
    "            tInfo = fp.readline().strip().split('-')[1]\n",
    "            tInfo = json.loads(tInfo)\n",
    "            if not termInfo:\n",
    "                termInfo = tInfo\n",
    "            else:\n",
    "                #Merge\n",
    "                termInfo['df'] += tInfo['df']\n",
    "                termInfo['wTf'].update(tInfo['wTf'])\n",
    "                termInfo['docIds'].extend(tInfo['docIds'])\n",
    "    return termInfo\n",
    "\n",
    "def getFpIdDict():\n",
    "    fpDict = {}\n",
    "    with open(f'{fileName}/docId.txt', \"r+\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split('>')\n",
    "            docId = line[0]\n",
    "            fpDict[int(docId)] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getUrl(docId, fpDict) -> str:\n",
    "    with open(f'{fileName}/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['url']\n",
    "    \n",
    "def getDocLen(docId, fpDict) -> str:\n",
    "    with open(f'{fileName}/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['docLen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# compute idf of a token  *idfs are the same for the same token, only depend on the df and N in docs\n",
    "def calculate_idf(df, N = 55382):\n",
    "    try:\n",
    "        idf = math.log((N) / (df))\n",
    "    except ZeroDivisionError as e:\n",
    "        idf = 0\n",
    "    return idf\n",
    "\n",
    "#comupte tfidf by tf and idf    \n",
    "def calculate_tfidf(tf, idf):   \n",
    "    return tf*idf\n",
    "\n",
    "#compute cosine_similarity bewteen query vector and doc vector,support any lenth\n",
    "def calculate_cosine_similarity(vector_query,vector_doc):\n",
    "    dot_product = sum(q * d for q, d in zip(vector_query, vector_doc))\n",
    "    norm_query = math.sqrt(sum(q * q for q in vector_query))\n",
    "    norm_doc = math.sqrt(sum(d * d for d in vector_doc))\n",
    "\n",
    "    return dot_product / ((norm_query * norm_doc)+0.1)\n",
    "\n",
    "#create query vector, query is like \"fox dog\", idf_dict is like {\"fox\":idf of fox,\"dog\":idf of dog}\n",
    "def create_query_vector(qWords, idf_dict):\n",
    "    query_vector = []\n",
    "\n",
    "    # Calculate TF-IDF for each word in the query\n",
    "    for word in qWords:\n",
    "        tf = qWords.count(word) / len(qWords)\n",
    "        idf = idf_dict[word]# Get IDF from the IDF dictionary (assuming it's already computed, you can just store df and N, and use calculate_idf to compute idf)\n",
    "        tf_idf = tf * idf\n",
    "        query_vector.append(tf_idf)\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "#create doc vectors, like {'doc1': [0.2556430078148932, 0.10177675964835226], 'doc2': [0.0, 0.30533027894505677]}\n",
    "def create_doc_vector(qWords, query_vector, inverted_index_tfs, idf_dict):\n",
    "    doc_vectors = {}\n",
    "\n",
    "    # Iterate over tokens in the query\n",
    "    # also replace with our tokenizors!!!\n",
    "    for token in qWords:\n",
    "        # Check if the token exists in the inverted index postings\n",
    "        if token in inverted_index_tfs:\n",
    "            postings = inverted_index_tfs[token]\n",
    "\n",
    "            # Iterate over the document IDs and TF values in the postings\n",
    "            for doc_id, tf in postings.items():\n",
    "                # Check if the document ID already has a vector\n",
    "                if doc_id not in doc_vectors:\n",
    "                    doc_vectors[doc_id] = [0] * len(query_vector)\n",
    "\n",
    "                # Set the TF-IDF value in the document vector based on the query vector index and IDF\n",
    "                query_index = qWords.index(token)\n",
    "                tfidf = tf * idf_dict[token]\n",
    "                doc_vectors[doc_id][query_index] = tfidf\n",
    "\n",
    "    return doc_vectors\n",
    "\n",
    "#compute cosine_similarity and return dictionary of {docid:cosine_similarity},already sorted, can make changes for like top 10\n",
    "def create_cs_doc(query_vector,doc_vectors):\n",
    "    doc_similarities={}\n",
    "    for docid,vetcor in doc_vectors.items():\n",
    "        doc_similarities[docid]=calculate_cosine_similarity(query_vector,vetcor)\n",
    "    doc_similarities = {k: v for k, v in sorted(doc_similarities.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def stemQuery(query:str) -> list:\n",
    "    stemmer = PorterStemmer()\n",
    "    queryList = list()\n",
    "    line = query.strip()\n",
    "    if line != '':\n",
    "        for aToken in re.split('[^a-z0-9]', line.lower()):\n",
    "            if (aToken != ''):\n",
    "                token = stemmer.stem(aToken)\n",
    "                queryList.append(token)\n",
    "    return queryList\n",
    "\n",
    "def getDocIds(queryWords:list, fpDict) -> set[int]:\n",
    "    unionSet = set()\n",
    "    for word in queryWords:\n",
    "        termInfo = getTermInfo(word,fpDict)\n",
    "        if termInfo:\n",
    "            docSet = set(termInfo['docIds'])\n",
    "            if len(unionSet) == 0:\n",
    "                unionSet = docSet\n",
    "            else:\n",
    "                unionSet = unionSet.intersection(docSet)\n",
    "    return unionSet\n",
    "\n",
    "def getIdfDict(queryList:list[str], fpDict, N = 55382) -> dict:\n",
    "    idfDict = {}\n",
    "    for word in queryList:\n",
    "        wordDf = getTermInfo(word,fpDict)['df']\n",
    "        idfDict[word] = calculate_idf(wordDf)\n",
    "    return idfDict\n",
    "\n",
    "def getInvertedTf(term:str, docIdList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    resultTf[term] = {}\n",
    "    tfDict = getTermInfo(term,fpDict)['wTf']\n",
    "    for docId in docIdList:\n",
    "        docIdStr = str(docId)\n",
    "        if docIdStr in tfDict:\n",
    "            resultTf[term][docIdStr] = tfDict[docIdStr] / getDocLen(docId, idDict)\n",
    "    return resultTf\n",
    "\n",
    "def getInvertedTfDict(queryList:list, docList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    for word in queryList:\n",
    "        resultTf.update(getInvertedTf(word,docList, fpDict, idDict))\n",
    "    return resultTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Index of Index before query\n",
    "fpDict = getFpDict(6)\n",
    "idDict = getFpIdDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo:\n",
    "# if union docIdList > 10\n",
    "    # remove docs with docLen <= 100\n",
    "    # consider ONLY high-idf query terms\n",
    "\n",
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22732 - 3565 https://www.ics.uci.edu/~ics1c/doc/html_crash.html 0.37230236899181274\n",
      "42188 - 704922 https://www.ics.uci.edu/~kay/courses/h22/hw/DVD-random.txt 0.10216737721380835\n",
      "25274 - 704922 https://www.ics.uci.edu/~kay/courses/h22/hw/DVD.txt 0.10216737721380835\n",
      "32946 - 48584 https://www.ics.uci.edu/~kay/courses/h22/hw/wordlist-random.txt 0.10086419940685792\n",
      "25237 - 48584 https://www.ics.uci.edu/~kay/courses/h22/hw/wordlist.txt 0.10086419940685792\n",
      "37727 - 3100 https://www.ics.uci.edu/~kay/courses/31/hw/lab6.html 0.06722716626996325\n",
      "7852 - 70946 http://flamingo.ics.uci.edu/releases/4.0/src/lbaktree/data/data.txt 0.04472487429720199\n",
      "6047 - 70946 http://flamingo.ics.uci.edu/releases/4.1/src/lbaktree/data/data.txt 0.04472487429720199\n",
      "46631 - 140272 https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/book1 0.03915812028882847\n",
      "49958 - 475851 https://www.ics.uci.edu/~kay/wordlist.txt 0.0040772218472512915\n",
      "85 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# qWords = stemQuery(\"machine learning\")  #Takes LONG AF\n",
    "qWords = stemQuery(\"fox dog and\") # ['fox', 'dog', 'and']\n",
    "docIds = getDocIds(qWords,fpDict) # {6047, 7852, 22732, 25237, 25274, 32946, 37727, 42188, 46631, 49958}\n",
    "idf_dict = getIdfDict(qWords,fpDict) # {'fox': 6.229746822993642, 'dog': 4.945407031631708, 'and': 0.5274289094373557}\n",
    "inverted_index_tfs = getInvertedTfDict(qWords, docIds, fpDict, idDict) #{docId: tf/DocLen}\n",
    "query_vector = create_query_vector(qWords,idf_dict) # [2.076582274331214, 1.6484690105439026, 0.17580963647911854]\n",
    "doc_vectors = create_doc_vector(qWords, query_vector, inverted_index_tfs, idf_dict) # {docId: vector[], ...}\n",
    "cs = create_cs_doc(query_vector,doc_vectors)\n",
    "for docIdStr in cs:\n",
    "    docUrl = getUrl(int(docIdStr), idDict)\n",
    "    print(docIdStr,'-',getDocLen(int(docIdStr), idDict), docUrl, cs[docIdStr])\n",
    "end = time.time()\n",
    "\n",
    "print(round((end - start) * 1000),'ms') #Time in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0768873426547767, 1.6485530002985138, 0.17584065381430652]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'49958': [0.00013093724775117276,\n",
       "  8.314634624528336e-05,\n",
       "  2.217172860592579e-06],\n",
       " '46631': [4.4418430106965974e-05,\n",
       "  0.0016218512179279891,\n",
       "  0.016137195851998746],\n",
       " '42188': [0.005046952737987512, 0.0005682875255312486, 0.001234014138329311],\n",
       " '22732': [0.017477312841414672, 0.01387281627179675, 0.007250652485470704],\n",
       " '7852': [0.0004391129893133038, 0.002300436205417541, 0.0002751150533277144],\n",
       " '32946': [0.0012824514300931028, 0.005395190331126785, 8.686348780551946e-05],\n",
       " '25237': [0.0012824514300931028, 0.005395190331126785, 8.686348780551946e-05],\n",
       " '37727': [0.0020098909767626875, 0.0015953738712566263, 0.01259245972476647],\n",
       " '25274': [0.005046952737987512, 0.0005682875255312486, 0.001234014138329311],\n",
       " '6047': [0.0004391129893133038, 0.002300436205417541, 0.0002751150533277144]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Sim: 0.9421524260705721\n",
      "Cosine Distance: 0.057847573929427853\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "#Data using slide 41 lec 21\n",
    "A = np.array([0.789, 0.515, 0.335,0])\n",
    "B = np.array([0.832, 0.557, 0, 0])\n",
    " \n",
    "# compute cosine Distnace\n",
    "cosineSim = np.dot(A,B)/(norm(A)*norm(B))\n",
    "cosineDistance = 1 - cosineSim\n",
    "print(\"Cosine Sim:\", cosineSim)\n",
    "print(\"Cosine Distance:\", cosineDistance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
