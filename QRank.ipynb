{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def getFpDict(count):\n",
    "    fpDict = {}\n",
    "    for i in range(1,count+1):\n",
    "        with open(f'Data/Posting{i}.txt', \"r+\") as fp:\n",
    "            while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "                fpPos = fp.tell()\n",
    "                line = fp.readline().split('-')\n",
    "                term = line[0]\n",
    "                if term not in fpDict:\n",
    "                    fpDict[term] = {}\n",
    "                fpDict[term][i] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getTermInfo(term:str, fpDict:dict):\n",
    "    termInfo = None\n",
    "    if term not in fpDict: return termInfo\n",
    "    for count in fpDict[term]:\n",
    "        with open(f'Data/Posting{count}.txt', \"r+\") as fp:\n",
    "            fp.seek(fpDict[term][count])\n",
    "            tInfo = fp.readline().strip().split('-')[1]\n",
    "            tInfo = json.loads(tInfo)\n",
    "            if not termInfo:\n",
    "                termInfo = tInfo\n",
    "            else:\n",
    "                #Merge\n",
    "                termInfo['df'] += tInfo['df']\n",
    "                termInfo['tf'].update(tInfo['tf'])\n",
    "                termInfo['docIds'].extend(tInfo['docIds'])\n",
    "    return termInfo\n",
    "\n",
    "\n",
    "def getFpIdDict():\n",
    "    fpDict = {}\n",
    "    with open(f'docId.txt', \"r+\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split()\n",
    "            docId = line[0]\n",
    "            fpDict[int(docId)] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getUrl(docId, fpDict) -> str:\n",
    "    with open(f'docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        return fp.readline().strip().split()[1]\n",
    "    \n",
    "def getDocLen(docId, fpDict) -> str:\n",
    "    with open(f'docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        return int(fp.readline().strip().split()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_idf(N, df):\n",
    "    idf = math.log((N + 0.1) / (df + 0.1))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def stemQuery(query:str) -> list:\n",
    "    stemmer = PorterStemmer()\n",
    "    queryList = list()\n",
    "    line = query.strip()\n",
    "    if line != '':\n",
    "        for aToken in re.split('[^a-z0-9]', line.lower()):\n",
    "            if (aToken != ''):\n",
    "                token = stemmer.stem(aToken)\n",
    "                queryList.append(token)\n",
    "    return queryList\n",
    "\n",
    "def getDocIds(queryWords:list, fpDict) -> set[int]:\n",
    "    unionSet = set()\n",
    "    for word in queryWords:\n",
    "        termInfo = getTermInfo(word,fpDict)\n",
    "        if termInfo:\n",
    "            docSet = set(termInfo['docIds'])\n",
    "            if len(unionSet) == 0:\n",
    "                unionSet = docSet\n",
    "            else:\n",
    "                unionSet = unionSet.intersection(docSet)\n",
    "    return unionSet\n",
    "\n",
    "def getIdfDict(queryList:list[str], fpDict, N = 55382) -> dict:\n",
    "    idfDict = {}\n",
    "    for word in queryList:\n",
    "        wordDf = getTermInfo(word,fpDict)['df']\n",
    "        idfDict[word] = calculate_idf(N, wordDf)\n",
    "    return idfDict\n",
    "\n",
    "def getInvertedTf(term:str, docIdList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    resultTf[term] = {}\n",
    "    tfDict = getTermInfo(term,fpDict)['tf']\n",
    "    for docId in docIdList:\n",
    "        docIdStr = str(docId)\n",
    "        if docIdStr in tfDict:\n",
    "            resultTf[term][f'doc{docId}'] = tfDict[docIdStr] / getDocLen(docId, idDict)\n",
    "    return resultTf\n",
    "\n",
    "def getInvertedTfDict(queryList:list, docList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    for word in queryList:\n",
    "        resultTf.update(getInvertedTf(word,docList, fpDict, idDict))\n",
    "    return resultTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Index of Index before query\n",
    "fpDict = getFpDict(3)\n",
    "idDict = getFpIdDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "qWords = stemQuery(\"fox dog and\") # ['fox', 'dog', 'and']\n",
    "docIds = getDocIds(qWords,fpDict) # {6047, 7852, 22732, 25237, 25274, 32946, 37727, 42188, 46631, 49958}\n",
    "idf_dict = getIdfDict(qWords,fpDict) # {'fox': 6.229746822993642, 'dog': 4.945407031631708, 'and': 0.5274289094373557}\n",
    "inverted_index_tfs = getInvertedTfDict(qWords, docIds, fpDict, idDict)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(round((end - start) * 1000),'ms') #Time in milliseconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
