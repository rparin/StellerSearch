{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def getFpDict(count):\n",
    "    fpDict = {}\n",
    "    for i in range(1,count+1):\n",
    "        with open(f'Data/Posting{i}.txt', \"r+\") as fp:\n",
    "            while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "                fpPos = fp.tell()\n",
    "                line = fp.readline().split('-')\n",
    "                term = line[0]\n",
    "                if term not in fpDict:\n",
    "                    fpDict[term] = {}\n",
    "                fpDict[term][i] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getTermInfo(term:str, fpDict:dict):\n",
    "    termInfo = None\n",
    "    if term not in fpDict: return termInfo\n",
    "    for count in fpDict[term]:\n",
    "        with open(f'Data/Posting{count}.txt', \"r+\") as fp:\n",
    "            fp.seek(fpDict[term][count])\n",
    "            tInfo = fp.readline().strip().split('-')[1]\n",
    "            tInfo = json.loads(tInfo)\n",
    "            if not termInfo:\n",
    "                termInfo = tInfo\n",
    "            else:\n",
    "                #Merge\n",
    "                termInfo['df'] += tInfo['df']\n",
    "                termInfo['wTf'].update(tInfo['wTf'])\n",
    "                termInfo['docIds'].extend(tInfo['docIds'])\n",
    "    return termInfo\n",
    "\n",
    "def getFpIdDict():\n",
    "    fpDict = {}\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        while not (fp.tell() == os.fstat(fp.fileno()).st_size):\n",
    "            fpPos = fp.tell()\n",
    "            line = fp.readline().split('>')\n",
    "            docId = line[0]\n",
    "            fpDict[int(docId)] = fpPos\n",
    "    return fpDict\n",
    "\n",
    "def getUrl(docId, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['url']\n",
    "    \n",
    "def getDocLen(docId, fpDict) -> str:\n",
    "    with open(f'Data/docId.txt', \"r+\") as fp:\n",
    "        fp.seek(fpDict[docId])\n",
    "        docInfo = json.loads(fp.readline().split('>')[1])\n",
    "        return docInfo['docLen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# compute idf of a token  *idfs are the same for the same token, only depend on the df and N in docs\n",
    "def calculate_idf(N, df):\n",
    "    idf = math.log((N + 0.1) / (df + 0.1))\n",
    "    return idf\n",
    "\n",
    "#comupte tfidf by tf and idf    \n",
    "def calculate_tfidf(tf, idf):   \n",
    "    return tf*idf\n",
    "\n",
    "#compute cosine_similarity bewteen query vector and doc vector,support any lenth\n",
    "def calculate_cosine_similarity(vector_query,vector_doc):\n",
    "    dot_product = sum(q * d for q, d in zip(vector_query, vector_doc))\n",
    "    norm_query = math.sqrt(sum(q * q for q in vector_query))\n",
    "    norm_doc = math.sqrt(sum(d * d for d in vector_doc))\n",
    "\n",
    "    return dot_product / ((norm_query * norm_doc)+0.1)\n",
    "\n",
    "#create query vector, query is like \"fox dog\", idf_dict is like {\"fox\":idf of fox,\"dog\":idf of dog}\n",
    "def create_query_vector(qWords, idf_dict):\n",
    "    query_vector = []\n",
    "\n",
    "    # Calculate TF-IDF for each word in the query\n",
    "    for word in qWords:\n",
    "        tf = qWords.count(word) / len(qWords)\n",
    "        idf = idf_dict[word]# Get IDF from the IDF dictionary (assuming it's already computed, you can just store df and N, and use calculate_idf to compute idf)\n",
    "        tf_idf = tf * idf\n",
    "        query_vector.append(tf_idf)\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "#create doc vectors, like {'doc1': [0.2556430078148932, 0.10177675964835226], 'doc2': [0.0, 0.30533027894505677]}\n",
    "def create_doc_vector(qWords, query_vector, inverted_index_tfs, idf_dict):\n",
    "    doc_vectors = {}\n",
    "\n",
    "    # Iterate over tokens in the query\n",
    "    # also replace with our tokenizors!!!\n",
    "    for token in qWords:\n",
    "        # Check if the token exists in the inverted index postings\n",
    "        if token in inverted_index_tfs:\n",
    "            postings = inverted_index_tfs[token]\n",
    "\n",
    "            # Iterate over the document IDs and TF values in the postings\n",
    "            for doc_id, tf in postings.items():\n",
    "                # Check if the document ID already has a vector\n",
    "                if doc_id not in doc_vectors:\n",
    "                    doc_vectors[doc_id] = [0] * len(query_vector)\n",
    "\n",
    "                # Set the TF-IDF value in the document vector based on the query vector index and IDF\n",
    "                query_index = qWords.index(token)\n",
    "                tfidf = tf * idf_dict[token]\n",
    "                doc_vectors[doc_id][query_index] = tfidf\n",
    "\n",
    "    return doc_vectors\n",
    "\n",
    "#compute cosine_similarity and return dictionary of {docid:cosine_similarity},already sorted, can make changes for like top 10\n",
    "def create_cs_doc(query_vector,doc_vectors):\n",
    "    doc_similarities={}\n",
    "    for docid,vetcor in doc_vectors.items():\n",
    "        doc_similarities[docid]=calculate_cosine_similarity(query_vector,vetcor)\n",
    "    doc_similarities = {k: v for k, v in sorted(doc_similarities.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return doc_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def stemQuery(query:str) -> list:\n",
    "    stemmer = PorterStemmer()\n",
    "    queryList = list()\n",
    "    line = query.strip()\n",
    "    if line != '':\n",
    "        for aToken in re.split('[^a-z0-9]', line.lower()):\n",
    "            if (aToken != ''):\n",
    "                token = stemmer.stem(aToken)\n",
    "                queryList.append(token)\n",
    "    return queryList\n",
    "\n",
    "def getDocIds(queryWords:list, fpDict) -> set[int]:\n",
    "    unionSet = set()\n",
    "    for word in queryWords:\n",
    "        termInfo = getTermInfo(word,fpDict)\n",
    "        if termInfo:\n",
    "            docSet = set(termInfo['docIds'])\n",
    "            if len(unionSet) == 0:\n",
    "                unionSet = docSet\n",
    "            else:\n",
    "                unionSet = unionSet.intersection(docSet)\n",
    "    return unionSet\n",
    "\n",
    "def getIdfDict(queryList:list[str], fpDict, N = 55382) -> dict:\n",
    "    idfDict = {}\n",
    "    for word in queryList:\n",
    "        wordDf = getTermInfo(word,fpDict)['df']\n",
    "        idfDict[word] = calculate_idf(N, wordDf)\n",
    "    return idfDict\n",
    "\n",
    "def getInvertedTf(term:str, docIdList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    resultTf[term] = {}\n",
    "    tfDict = getTermInfo(term,fpDict)['wTf']\n",
    "    for docId in docIdList:\n",
    "        docIdStr = str(docId)\n",
    "        if docIdStr in tfDict:\n",
    "            resultTf[term][docIdStr] = tfDict[docIdStr] / getDocLen(docId, idDict)\n",
    "    return resultTf\n",
    "\n",
    "def getInvertedTfDict(queryList:list, docList:list[int], fpDict, idDict) -> dict:\n",
    "    resultTf = {}\n",
    "    for word in queryList:\n",
    "        resultTf.update(getInvertedTf(word,docList, fpDict, idDict))\n",
    "    return resultTf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Index of Index before query\n",
    "fpDict = getFpDict(6)\n",
    "idDict = getFpIdDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ics.uci.edu/~ics1c/doc/html_crash.html 0.3722521606742945\n",
      "https://www.ics.uci.edu/~kay/courses/h22/hw/DVD-random.txt 0.10214257113753401\n",
      "https://www.ics.uci.edu/~kay/courses/h22/hw/DVD.txt 0.10214257113753401\n",
      "https://www.ics.uci.edu/~kay/courses/h22/hw/wordlist-random.txt 0.10085162388553048\n",
      "https://www.ics.uci.edu/~kay/courses/h22/hw/wordlist.txt 0.10085162388553048\n",
      "https://www.ics.uci.edu/~kay/courses/31/hw/lab6.html 0.06721793450252492\n",
      "http://flamingo.ics.uci.edu/releases/4.0/src/lbaktree/data/data.txt 0.0447191445084951\n",
      "http://flamingo.ics.uci.edu/releases/4.1/src/lbaktree/data/data.txt 0.0447191445084951\n",
      "https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/book1 0.03915725605622696\n",
      "https://www.ics.uci.edu/~kay/wordlist.txt 0.004076291004745179\n",
      "139 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# qWords = stemQuery(\"machine learning\")  #Takes LONG AF\n",
    "qWords = stemQuery(\"fox dog and\") # ['fox', 'dog', 'and']\n",
    "docIds = getDocIds(qWords,fpDict) # {6047, 7852, 22732, 25237, 25274, 32946, 37727, 42188, 46631, 49958}\n",
    "idf_dict = getIdfDict(qWords,fpDict) # {'fox': 6.229746822993642, 'dog': 4.945407031631708, 'and': 0.5274289094373557}\n",
    "inverted_index_tfs = getInvertedTfDict(qWords, docIds, fpDict, idDict)\n",
    "query_vector = create_query_vector(qWords,idf_dict) # [2.076582274331214, 1.6484690105439026, 0.17580963647911854]\n",
    "doc_vectors = create_doc_vector(qWords, query_vector, inverted_index_tfs, idf_dict)\n",
    "cs = create_cs_doc(query_vector,doc_vectors)\n",
    "for docIdStr in cs:\n",
    "    docUrl = getUrl(int(docIdStr), idDict)\n",
    "    print(docUrl, cs[docIdStr])\n",
    "end = time.time()\n",
    "\n",
    "print(round((end - start) * 1000),'ms') #Time in milliseconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
